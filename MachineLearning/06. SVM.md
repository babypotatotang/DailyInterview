# **SVM: Support Vector Machine**

## **Basic Notion**
* a supervised machine learning algorithm that helps in classification or regression problems. 
* normally for binary classification 
* Category
    - Linearity 
        - Linear svm
        - Non-Linear svm
    - Missclassification
        - (accepted) soft margin svm
        - (upaccepted) hard margin svm

* What is the best svm model? 
    - Robust : less affected by outliers
    - It aims to find an optimal boundary between the possible outputs. 

* Terms
    1. Hyperplane : A boundary that devides the data
    2. Support Vector : The closest data with Hyperplane
    3. Margin : A distance between hyperplane and support vector

## **Soft Margin SVM**
* Accept the missclassfication with penalty so that the total error can be minimized
* How to give penalty
    * 0-1 loss
        * Calculate Penaly as much as error happend
    * Hinge loss
        * Differ error size according to the missclassification 

## **Non-Linear SVM**
* There's no Linearline to classify and can't avoid outlier. 
* **Solution**: Mapping to higher dimension, Find hyperplane not in the original vector space but in the feature space. 
### **Kernel Trick**
* Kernel
    * Mapping Function of the data 
    * a method used to take data as input and transform it into the required form of processing data. 
    * with Lagrange, mapping to a higher dimension increased the dimension of the inner product and increases the amount of computation. 
    * In real applications, there might be many features in the data and applying transformations that involve many polynomial combinations of these features will lead to extremely high and impractical computational costs.    
* **Kernel Trick**
    * Allow us to operate in the original feature space without computing the coordinates of the data in a higher dimensional space. 
    * (The most Popular) RBF kernel: infinite number of dimensions in the feature space because it can be expanded by the Taylor Series.
## **Multi Class SVM**
### **One VS One**
* When there are N classes, binary classification is made 1:1 for all classes, and the winner is decided by voting. 
* Each class has its own classifier, so we need n*(n-1)/2 classifiers. 
* Thre are A,B,C
    1. Machine devides whether A or B
    2. Machine devides whether B or C
    3. Machine devides whether A or C

### **One VS Rest**
* When there are N classes, binary classification is made 1:(N-1) for all classes, and decide wheter this class is right or not. 
* We need N classifiers. 
* There are A,B,C
    1. Machine devides whether A or not A
    2. Machine devides whether B or not B
    3. Machine devides whether C or not C
