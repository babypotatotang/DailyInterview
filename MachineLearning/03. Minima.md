## **Local Minima와 Global Minima에 대해 설명해주세요.**

### **개념**
- Local Minima
    - (함수의 극소점)
    - 주위의 모든 점의 함숫값 이하의 함숫값을 갖는 점, 극점
- Global Minima
    - (함수의 최소점)
    - 정의역의 모든 점의 함숫값 이하의 함숫값을 갖는 점.
- Local Minima 문제
    - 에러를 최소화시키는데 최적의 파라미터를 찾는 문제에 있어서 파라미터 공간에 수많은 지역적인 hole들이 존재하여 이러한 local minima에 빠질 경우 전역적인 해 (global minima)를 찾기 힘들게 되는 문제를 일컫는다.
    - 기존에 ml이 잘 안되거나 성능이 잘 안나오는 이유는 학습 도중 이러한 local minima에 빠졌기 때문이라는 상식
    - 그러나, 최근 local minima 문제는 사실 고차원의 공간에서 발생하기 힘든 희귀한 경우라고 주장함.
- 이런 문제를 방지하기 위한 다양한 Optimize 기법이 있음.
    - SGD(확률적 경사하강법)
        - 전체 입력 데이터로 가중치와 편향이 업데이트되는 것이 아니라 일부 데이터만 사용
        - 전체 x,y데이터에서 랜덤하게 배치 사이즈만큼 추출하는데 이를 mini batch
        - 빠른 학습 속도, 메모리 절약
    - Momentum(모멘텀)
        - SGD의 높은 편차를 줄이고 수렴을 부드럽게 하기 위해 고안
        - 관련 방향으로의 수렴을 가속화하고 관련없는 방향으로의 변동을 줄이는 관성을 줌.
    - NAG (Nesterov Accelerated Gradient_
        - Momentum에서 개선된 알고리즘으로 차이점은 Gradient 값 없이 관성에 의해서만 이동한 값을 계산하는 것
        - Momentum 방식을 베이스로 하며 미리 경사를 확인하고, 그에 맞춰 속도를 조절하는 방식으로 기울기를 계산

### **요약**
Local Minima는 함수의 극소점으로 주위의 모든 점의 함숫값 이하의 함숫값을 갖는 점이고, Global Minima는 정의역 모든 점의 함숫값 이하의 함숫값을 갖는 점으로 머신러닝 시 Global Minima를 찾는 것이 최우선되는 목표입니다. Local minima에 빠질 경우 Global Minima를 찾기 힘들게 되는 문제가 발생하여 이를 해결하기 위해 SGD, Momentum등의 최적화 기법이 존재합니다. 