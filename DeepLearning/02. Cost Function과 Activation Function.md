## **CostFunction과 Activation Function의 차이는 무엇인가요?**
### **개념**
- **Cost Function**
    - 최적화된(정답값과 예측값 간의 오차가 가장 작은) 가설함수를 도출하기 위해 사용되는 함수.
    - 가설합수의 형태를 결정짓는 parameter(매개변수)를 적절하게 조정할 수 있는 정보를 제공하는 함수.
- **Activation Function**
    - 입력신호의 총합을 출력신호로 변환하는 함수
    - Activation Function 노드는 입력 또는 입력 신호의 총합이 주어졌을때, 출력신호를 결정지음.
    - 선형회귀 혹은 분류 문제에서 활성함수는 $g(z)=z$
    - 대부분의 경우 비선형 함수를 사용함
        - ‘s’ 자 함수($sigmoid$ 등)을 사용하는 경우 output은 (0,1), (-1,1)의 범위를 가지고 있으며, *Yes/No와 같은 binary decision*에 사용됨.
            - 이때 사용되는 함수는 다음의 성질을 가지고 있음. **(non-saturating function)**
                - 양과 음의 방향으로 어떤 값에 수렴함.
                - 즉, 데이터의 범위가 있음.
            - 이 경우 **vanishing gradient problem**이 발생될 수 있음.
        - 위의 문제를 해결하기 위한 **saturating sigmoidal activation function**으로는 **ReLU**가 있음.
            - **ReLU**: $$f(x)=max(0,x)$$
### **정리**
CostFunction은 정답값과 예측값 간의 오차가 가장 작은 가설함수를 도출하기 위해 사용되는 함수이고, Activation Function은 입력 신호를 출력신호의 형태로 변환하는 함수입니다. 경우에 따라 다르게 사용되는 Activation Function을 통해 보다 효율적인 gradient를 설정할 수 있습니다. 