# **Vanishing Gradient**

* It is encountered when training AI NN with **gradient-based learning methods and backpropagation.** 
* The gradient will be vanishingly small, effectively preventing the weight from changing its value. 
* This may completely stop the nn from further training. 
* Example
    * Traditional activation functions such as the hyperbolic tangent function have gradinets in the range (0,1]
    * The backpropogation computes gradients by the chain rule. 
        * **chain rule**
    * This has the effect of multiplying *n* of these small numbers to compute gradients of the early layers in an n-layer network, meaning that the gradient(error signal) decreases exponentially with n while the early layers train very slowly. 

## **final**
* As more layers using certain activation functions are added to neural networks, the gradients of the loss function approaches zero, making the network hard to train. 
* (why) Certain activation functions, like the sigmoid function, squishes a large input space into a small input space between 0 and 1. 
* (solution 1) Use **other** activation functions. 
* (solution 2) **Residual networks**: They directly add the value at the beginning of the block, to the end of the block so that they don't go through activation functions that squash the derivates, resulting in a higher overall derivative of the block. 