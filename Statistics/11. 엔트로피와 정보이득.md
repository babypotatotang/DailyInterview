## **엔트로피(entropy)에 대해 설명해주세요. 가능하면 Information Gain도요.**

### **개념**
#### **엔트로피**
* 주어진 데이터 집합의 혼잡도를 의미
* 주어진 데이터 집합에서 서로 다른 종류의 레코드가 섞여있으면 엔트로피가 높고, 같은 종류의 레코드가 섞여 있으면 엔트로피가 낮음.
* 0에서 1사이의 값을 가지며, 가장 혼잡도가 높은 상태가 1이고, 반대는 0임(하나의 레코드로만 구성된 경우)
* **계산 방법**
$$entropy(S) = - \sum_{i=1}^m p_ilog_2(p_i), \: p_i =  \frac{freq(C_i,S)}{|S|}$$
* S: 데이터들의 집합, C: 클래스 값들의 집합, freq(Ci,S): S에서 Ci가 속한 레코드의 수, |S|: S의 데이터 개수
* 쉽게 말하면, 레코드 값의 포함 비율($p_i$)와 $p_i$에 로그를 적용한 가중치를 곱한값을 모두 더하는 식 
* 원래는 $log_2 \frac{1}{p_i}$ 를 계산하는 것으로, 로그 성질에 의해 -를 붙임. 
#### **정보 이득**
* 결정 트리에서 엔트로피를 계산하고, 어떤 노드를 선택하는 것이 옳은지 따져볼대 사용하는 기대값 
* 어떤 속성을 선택함으로 인해서 데이터를 더 잘 구분하게 되는 것
* 학생 데이터에서 수능등급을 구분할때, 수학 점수가 체육 점수보다 변별력이 더 높다고 가정, 그러면 수학 점수 속성이 체육 점수 속성보다 정보이득이 높음. 
* 부모노드의 정보량에서 자식도의 정보량을 뺀 것
* 공식
    * Information Gain = (Entropy before split) - (weighted entropy after split) 
    * 이를 통해 어떤 속성을 선택할지 정할 수 있음. 
    * 값이 클수록 유의미 
* **계산 방법**
$$Gain(A) = I(s_1,s_2,s_3, ... , s_m) - E(속성 A)$$
* s1,s2 .. 는 상위 노드의 엔트로피
* 쉽게 말하면, 상위 노드의 엔트로피에서 하위노드의 엔트로피를 뺀 
* E(A)는 속성 A의 하위 노드의 엔트로피를 평균한 값
* Gain(A)는 속성 A를 선택했을때 정보 이득 양을 계산하는 수식으로 원래 노드의 엔트로피를 구해야함.

### **요약**
엔트로피는 주어진 데이터집합이 얼마나 혼잡한지를 알려주는 척도로, 0에서 1사이의 값을 가지며 1에 가까울 수록 혼잡도가 높은 상태입니다. 엔트로피가 0인 경우 전체 집합이 하나의 데이터로만 존재한다고 말할 수 있습니다. 


정보이득은 엔트로피를 통해 어떤 속성을 선택할지 결정하는 척도로, 주로 결정트리에서 사용됩니다. 결정트리에서 정보이득을 통해 어떻게 데이터를 split할지를 결정할 수 있습니다. 