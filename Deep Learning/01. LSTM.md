# **LSTM**
First of all, I will explain in the view of NLP. 

## **Background**
* In case of RNN, we only have the hidden state to understand the context of text.
* When it comes to a long text, with lots of hidden states, there are **long-term dependency** problem. 

    **Long-Term Dependency**
    - The further the distance between input and output, the less correlation.
    - When the input and output gets far, their relationship is going to be less. 
    - In theory, RNNs are capable of handling such "long-term dependencies". In practice, RNN don't seem to be able to learn them.
    - The problem, **Vanishing gradient problem** is going to be happen. 
    - The nn update the weight using the gradient descent algorithm. 
    - (Vanishing Gradient Problem) The difference in the gradients is very small, network will not learn anything and so no difference in the output. 

* LSTMa are a **special kind of RNN**, capable of learning long-term dependencies. 
* **Remembering information for long periods of time** is practically their default behavior. 

## **The Core Idea Behind LSTMs**
* the cell state
    * horizontal line running through the top of the diagram. 
    * runs straight down the entire chain, with only some minor linear interactions.
* Gates are composed out of a **sigmoid neural net layer** and a **pointwise multiplication operation**.
    * Sigmoid Layer
        - outputs numbers between 0 and 1, describing how much of each component should be let through.
    * Zero means "let nothing through" while One means "let everything through".

## **Step by Step**
* **forget gate layer**
    * Look at $h_{t-1}$ and $x_t$ and outputs a number between 0 and 1 for each number in the cell state $C_{t-1}$. 
    * A 1 represents "completely keep this" while a 0 represents "completely get rid of this". 
* **input gate layer**
    * Decide which values we'll updat. 
    1. sigmoid layer decide which value to update
    2. tanh layer creates a vector of new candidata values. 

## **pros and cons**
* **pros**
    * They are capable of learning long-term dependencies, especially in sequence prediction problems. 
    * They can solve the problem of vanishing gradients. 
* **cons**
    * They can't remove the vanishing gradients problem completely.
    * They require a lot of resources and time to get trained and become ready for real-world applications. 
    * With the rise of data mining, the model should remember past information for a longer time than LSTMs. 